{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWtDrZB6S4DC"
      },
      "source": [
        "\n",
        "# Quantization aware training in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-0OhISS4DF"
      },
      "source": [
        "### Librerias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_tOJgNOUUqd",
        "outputId": "52327962-0332-42e8-bf37-03651cbd03f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.21.6)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sOqNBKZhS4DG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfmot.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B5qefIsVReY",
        "outputId": "5d71eaee-3e44-4d23-b85b-279513639636"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73MLABMxS4DH"
      },
      "source": [
        "### Carga datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ldwcYoR6S4DH"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# DATOS\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT69Z34XS4DH"
      },
      "source": [
        "### Creaci√≥n de modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7lB-5swNS4DI"
      },
      "outputs": [],
      "source": [
        "# dimension de dominio intermedio\n",
        "encoding_dim = 32  \n",
        "\n",
        "encoder_input = Input(shape=(784,), name=\"original_img\")\n",
        "# ENCODER\n",
        "encoded = Dense(256, activation='relu')(encoder_input)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoder_out = Dense(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "# DECODER\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "decoded = Dense(64, activation='relu')(encoded_input)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoder_output = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "# Modelo para el encoder\n",
        "encoder = Model(encoder_input, encoder_out, name=\"encoder\")\n",
        "\n",
        "# Modelo del decoder\n",
        "decoder = Model(encoded_input, decoder_output, name=\"decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyafxMEAS4DJ",
        "outputId": "a3d9c8de-7033-4bba-e3f9-89d461956037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " img (InputLayer)            [(None, 784)]             0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 32)                244215    \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 784)               244967    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 489,182\n",
            "Trainable params: 489,136\n",
            "Non-trainable params: 46\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "q_encoder_model = quantize_model(encoder)\n",
        "q_decoder_model = quantize_model(decoder)\n",
        "\n",
        "# AUTOENCODER\n",
        "autoencoder_input = Input(shape=(784,), name=\"img\")\n",
        "encoded_img = q_encoder_model(autoencoder_input)\n",
        "decoded_img = q_decoder_model(encoded_img)\n",
        "autoencoder = Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
        "\n",
        "autoencoder.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "                    loss='mse')\n",
        "\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qrUFKqS4DJ"
      },
      "source": [
        "En caso de autoencoder requiere quantizar encoder y decoder de forma separada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxRuda2HS4DK"
      },
      "source": [
        "### Entrenamiento cuantizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIHA-qetS4DK"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Im4nk6fmS4DK"
      },
      "outputs": [],
      "source": [
        "my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=50),\n",
        "                # tf.keras.callbacks.ModelCheckpoint(filepath='./quant_models/quant_aware_dense.h5'),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=1, min_delta=1e-8, mode='min')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVF7zoMiS4DL",
        "outputId": "2fc9ddf7-c4df-4cfb-fc9d-c268b2abab01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1875/1875 [==============================] - 23s 9ms/step - loss: 0.0394 - val_loss: 0.0299 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0283 - val_loss: 0.0276 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0294 - val_loss: 0.0291 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0332 - val_loss: 0.0368 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0333 - val_loss: 0.0272 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0258 - val_loss: 0.0245 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0231 - val_loss: 0.0220 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0217 - val_loss: 0.0209 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0207 - val_loss: 0.0202 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0198 - val_loss: 0.0195 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0191 - val_loss: 0.0186 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0184 - val_loss: 0.0179 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0178 - val_loss: 0.0172 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0172 - val_loss: 0.0168 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0166 - val_loss: 0.0164 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0160 - val_loss: 0.0159 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0155 - val_loss: 0.0150 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0151 - val_loss: 0.0148 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0148 - val_loss: 0.0147 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0144 - val_loss: 0.0142 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0141 - val_loss: 0.0141 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0138 - val_loss: 0.0142 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0136 - val_loss: 0.0133 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0134 - val_loss: 0.0133 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0132 - val_loss: 0.0131 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0130 - val_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0129 - val_loss: 0.0129 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0127 - val_loss: 0.0130 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0126 - val_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0125 - val_loss: 0.0125 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0123 - val_loss: 0.0121 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0122 - val_loss: 0.0126 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0121 - val_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0120 - val_loss: 0.0120 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0119 - val_loss: 0.0124 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0118 - val_loss: 0.0118 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0117 - val_loss: 0.0119 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0116 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0116 - val_loss: 0.0115 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0115 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0114 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0114 - val_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0114 - val_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0113 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0114 - val_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0113 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0113 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0112 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0112 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0111 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0111 - val_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0110 - val_loss: 0.0111 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0110 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0110 - val_loss: 0.0114 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0110 - val_loss: 0.0113 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0110 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0110 - val_loss: 0.0109 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0110 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0109 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0109 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0109 - val_loss: 0.0112 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.0109\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0109 - val_loss: 0.0110 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0098 - val_loss: 0.0101 - lr: 2.5000e-04\n",
            "Epoch 64/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0099 - val_loss: 0.0102 - lr: 2.5000e-04\n",
            "Epoch 65/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0099 - val_loss: 0.0100 - lr: 2.5000e-04\n",
            "Epoch 66/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0099 - val_loss: 0.0101 - lr: 2.5000e-04\n",
            "Epoch 67/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0099 - val_loss: 0.0104 - lr: 2.5000e-04\n",
            "Epoch 68/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0099 - val_loss: 0.0103 - lr: 2.5000e-04\n",
            "Epoch 69/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0099 - val_loss: 0.0103 - lr: 2.5000e-04\n",
            "Epoch 70/1000\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0098\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0098 - val_loss: 0.0106 - lr: 2.5000e-04\n",
            "Epoch 71/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0094 - val_loss: 0.0098 - lr: 6.2500e-05\n",
            "Epoch 72/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0095 - val_loss: 0.0102 - lr: 6.2500e-05\n",
            "Epoch 73/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0095 - val_loss: 0.0099 - lr: 6.2500e-05\n",
            "Epoch 74/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0095 - val_loss: 0.0101 - lr: 6.2500e-05\n",
            "Epoch 75/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0095 - val_loss: 0.0100 - lr: 6.2500e-05\n",
            "Epoch 76/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0095\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0095 - val_loss: 0.0099 - lr: 6.2500e-05\n",
            "Epoch 77/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0093 - val_loss: 0.0095 - lr: 1.5625e-05\n",
            "Epoch 78/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - val_loss: 0.0097 - lr: 1.5625e-05\n",
            "Epoch 79/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - val_loss: 0.0097 - lr: 1.5625e-05\n",
            "Epoch 80/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0094 - val_loss: 0.0097 - lr: 1.5625e-05\n",
            "Epoch 81/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0094 - val_loss: 0.0098 - lr: 1.5625e-05\n",
            "Epoch 82/1000\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.0094\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0094 - val_loss: 0.0096 - lr: 1.5625e-05\n",
            "Epoch 83/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - val_loss: 0.0097 - lr: 3.9063e-06\n",
            "Epoch 84/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - val_loss: 0.0096 - lr: 3.9063e-06\n",
            "Epoch 85/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0093 - val_loss: 0.0096 - lr: 3.9063e-06\n",
            "Epoch 86/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0093 - val_loss: 0.0097 - lr: 3.9063e-06\n",
            "Epoch 87/1000\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0093\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0093 - val_loss: 0.0098 - lr: 3.9063e-06\n",
            "Epoch 88/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0091 - val_loss: 0.0096 - lr: 9.7656e-07\n",
            "Epoch 89/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - val_loss: 0.0096 - lr: 9.7656e-07\n",
            "Epoch 90/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0092 - val_loss: 0.0098 - lr: 9.7656e-07\n",
            "Epoch 91/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0093 - val_loss: 0.0098 - lr: 9.7656e-07\n",
            "Epoch 92/1000\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0093\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0093 - val_loss: 0.0099 - lr: 9.7656e-07\n",
            "Epoch 93/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0091 - val_loss: 0.0096 - lr: 2.4414e-07\n",
            "Epoch 94/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - val_loss: 0.0096 - lr: 2.4414e-07\n",
            "Epoch 95/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0092 - val_loss: 0.0095 - lr: 2.4414e-07\n",
            "Epoch 96/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0092 - val_loss: 0.0097 - lr: 2.4414e-07\n",
            "Epoch 97/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - val_loss: 0.0097 - lr: 2.4414e-07\n",
            "Epoch 98/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - val_loss: 0.0097 - lr: 2.4414e-07\n",
            "Epoch 99/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0093 - val_loss: 0.0096 - lr: 2.4414e-07\n",
            "Epoch 100/1000\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0093\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0093 - val_loss: 0.0097 - lr: 2.4414e-07\n",
            "Epoch 101/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0091 - val_loss: 0.0097 - lr: 6.1035e-08\n",
            "Epoch 102/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0092 - val_loss: 0.0097 - lr: 6.1035e-08\n",
            "Epoch 103/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0092 - val_loss: 0.0095 - lr: 6.1035e-08\n",
            "Epoch 104/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0092 - val_loss: 0.0098 - lr: 6.1035e-08\n",
            "Epoch 105/1000\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0092\n",
            "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0092 - val_loss: 0.0095 - lr: 6.1035e-08\n",
            "Epoch 106/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0090 - val_loss: 0.0094 - lr: 1.5259e-08\n",
            "Epoch 107/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0091 - val_loss: 0.0094 - lr: 1.5259e-08\n",
            "Epoch 108/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0091 - val_loss: 0.0094 - lr: 1.5259e-08\n",
            "Epoch 109/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0091 - val_loss: 0.0094 - lr: 1.5259e-08\n",
            "Epoch 110/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0091 - val_loss: 0.0096 - lr: 1.5259e-08\n",
            "Epoch 111/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0091 - val_loss: 0.0095 - lr: 1.5259e-08\n",
            "Epoch 112/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0091 - val_loss: 0.0095 - lr: 1.5259e-08\n",
            "Epoch 113/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0091 - val_loss: 0.0096 - lr: 1.5259e-08\n",
            "Epoch 114/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0091\n",
            "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0091 - val_loss: 0.0096 - lr: 1.5259e-08\n",
            "Epoch 115/1000\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 116/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 117/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0093 - lr: 3.8147e-09\n",
            "Epoch 118/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 119/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 120/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 121/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 122/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 123/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 124/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 125/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 126/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0088 - val_loss: 0.0093 - lr: 3.8147e-09\n",
            "Epoch 127/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 128/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0088\n",
            "Epoch 128: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0088 - val_loss: 0.0092 - lr: 3.8147e-09\n",
            "Epoch 129/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 130/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 131/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 132/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 133/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0092 - lr: 9.5367e-10\n",
            "Epoch 134/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 135/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 136/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0092 - lr: 9.5367e-10\n",
            "Epoch 137/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 138/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 139/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 140/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0087\n",
            "Epoch 140: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 9.5367e-10\n",
            "Epoch 141/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0087 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 142/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 143/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 144/1000\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 145/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 146/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 147/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 148/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 149/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0086\n",
            "Epoch 149: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3842e-10\n",
            "Epoch 150/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.9605e-11\n",
            "Epoch 151/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.9605e-11\n",
            "Epoch 152/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.9605e-11\n",
            "Epoch 153/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.9605e-11\n",
            "Epoch 154/1000\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 154: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.9605e-11\n",
            "Epoch 155/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4901e-11\n",
            "Epoch 156/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4901e-11\n",
            "Epoch 157/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4901e-11\n",
            "Epoch 158/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4901e-11\n",
            "Epoch 159/1000\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 159: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4901e-11\n",
            "Epoch 160/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.7253e-12\n",
            "Epoch 161/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.7253e-12\n",
            "Epoch 162/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.7253e-12\n",
            "Epoch 163/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.7253e-12\n",
            "Epoch 164/1000\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 164: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.7253e-12\n",
            "Epoch 165/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 166/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 167/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 168/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 169/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 170/1000\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.3132e-13\n",
            "Epoch 171/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3283e-13\n",
            "Epoch 172/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3283e-13\n",
            "Epoch 173/1000\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3283e-13\n",
            "Epoch 174/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3283e-13\n",
            "Epoch 175/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0086\n",
            "Epoch 175: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.3283e-13\n",
            "Epoch 176/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.8208e-14\n",
            "Epoch 177/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.8208e-14\n",
            "Epoch 178/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.8208e-14\n",
            "Epoch 179/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.8208e-14\n",
            "Epoch 180/1000\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 180: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.8208e-14\n",
            "Epoch 181/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4552e-14\n",
            "Epoch 182/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4552e-14\n",
            "Epoch 183/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4552e-14\n",
            "Epoch 184/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4552e-14\n",
            "Epoch 185/1000\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 185: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4552e-14\n",
            "Epoch 186/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.6380e-15\n",
            "Epoch 187/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.6380e-15\n",
            "Epoch 188/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.6380e-15\n",
            "Epoch 189/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.6380e-15\n",
            "Epoch 190/1000\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 190: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.6380e-15\n",
            "Epoch 191/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.0949e-16\n",
            "Epoch 192/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.0949e-16\n",
            "Epoch 193/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.0949e-16\n",
            "Epoch 194/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.0949e-16\n",
            "Epoch 195/1000\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 195: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 9.0949e-16\n",
            "Epoch 196/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.2737e-16\n",
            "Epoch 197/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.2737e-16\n",
            "Epoch 198/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.2737e-16\n",
            "Epoch 199/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.2737e-16\n",
            "Epoch 200/1000\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0086\n",
            "Epoch 200: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 2.2737e-16\n",
            "Epoch 201/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.6843e-17\n",
            "Epoch 202/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.6843e-17\n",
            "Epoch 203/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.6843e-17\n",
            "Epoch 204/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.6843e-17\n",
            "Epoch 205/1000\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 5.6843e-17\n",
            "Epoch 206/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4211e-17\n",
            "Epoch 207/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4211e-17\n",
            "Epoch 208/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4211e-17\n",
            "Epoch 209/1000\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4211e-17\n",
            "Epoch 210/1000\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 210: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 1.4211e-17\n",
            "Epoch 211/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.5527e-18\n",
            "Epoch 212/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.5527e-18\n",
            "Epoch 213/1000\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.5527e-18\n",
            "Epoch 214/1000\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.5527e-18\n",
            "Epoch 215/1000\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0086\n",
            "Epoch 215: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0086 - val_loss: 0.0091 - lr: 3.5527e-18\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20aa565ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=1000,\n",
        "                #batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks = my_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZYO_XJFnS4DL"
      },
      "outputs": [],
      "source": [
        "# autoencoder.load_weights('./quant_models/quant_aware_dense.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_7CYs6nS4DL"
      },
      "source": [
        "### Convert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_pRQlHGS4DL",
        "outputId": "f5005811-e8da-4173-a675-486ed0a3d23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgahurmn3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpgahurmn3/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500280"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tflite_model_path = \"./quant_aware_dense.tflite\"\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "open(tflite_model_path, \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj3kNHTaS4DM"
      },
      "source": [
        "### Charge and predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNkM0nENS4DM"
      },
      "outputs": [],
      "source": [
        "batch = 1\n",
        "\n",
        "tflite_interpreter = tf.lite.Interpreter(model_path = tflite_model_path)\n",
        "\n",
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "tflite_interpreter.resize_tensor_input(input_details[0]['index'], (batch, 784))\n",
        "tflite_interpreter.resize_tensor_input(output_details[0]['index'], (batch, 784))\n",
        "tflite_interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", input_details[0]['name'])\n",
        "print(\"shape:\", input_details[0]['shape'])\n",
        "print(\"type:\", input_details[0]['dtype'])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", output_details[0]['name'])\n",
        "print(\"shape:\", output_details[0]['shape'])\n",
        "print(\"type:\", output_details[0]['dtype'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsTKhs_qS4DM"
      },
      "outputs": [],
      "source": [
        "def iterate(array, batch):\n",
        "    inf = 0\n",
        "    sup = batch\n",
        "    while sup < len(array)+batch:\n",
        "        res = array[inf:sup, :]\n",
        "        yield res\n",
        "        inf += batch\n",
        "        sup += batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2nlKsqS4DM"
      },
      "outputs": [],
      "source": [
        "tflite_interpreter = tf.lite.Interpreter(model_path = tflite_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lCGG85CS4DN"
      },
      "outputs": [],
      "source": [
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "t1 = time.time()\n",
        "iterator_test = iterate(x_test, 1)\n",
        "results = []\n",
        "\n",
        "#while True:\n",
        "#    try:    \n",
        "tflite_interpreter.set_tensor(input_details, next(iterator_test))\n",
        "\n",
        "tflite_interpreter.invoke()\n",
        "\n",
        "tflite_model_predictions = tflite_interpreter.get_tensor(output_details)\n",
        "results.append(tflite_model_predictions)\n",
        "#    except:\n",
        "#        break\n",
        "t2 = time.time()\n",
        "print(f\"Execute time: {round(t2 - t1, 2)} segs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JPz8etrS4DN"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xsci7IvqS4DN"
      },
      "outputs": [],
      "source": [
        "quant_decoded = autoencoder.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxXQhcj6S4DO"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "ZjGlojv5S4DO",
        "outputId": "7fde7b6f-64f0-4a65-db63-ae02f461eb6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "n = 10 \n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruccion\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(quant_decoded[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "183bbf6827d058c2a2fb0f4acdc0420849dda2b4380af0e437e38c64d798d8b7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Quantization aware training in Keras.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}